# 前言

学过的东西只记得名字了，这里重写一遍笔记。

## 符号说明

原版符号定义中，$x^{(i)}$与$x_i$ 存在混用的情况，请注意识别。
### **数据标记与上下标**

- 上标$^{(i)}$代表第$i$个训练样本
- 上标$^{[l]}$代表第$l$层 
-  m数据集的样本数
- 下标$_x$输入数据
- 下标$_y$输出数据
-  $n_x$输入大小
-  $n_x$输出大小 (或者类别数)
-  $n_h^{[l]}$第$l$层的隐藏单元数
-  $L$神经网络的层数
- 在循环中
	- $n_x=n_h^{[0]}$
	- $n_y=n_h^{[L+1]}$

### **神经网络模型**

-  $X{\in}\mathbb{R^{n_x\times m}}$代表输入的矩阵
-  $x^{(i)}{\in}\mathbb{R^{n_x}}$代表第$i$个样本的列向量
-  $Y{\in}\mathbb{R^{n_y\times m}}$是标记矩阵
-  $y^{(i)}{\in}\mathbb{R^{n_y}}$是第$i$样本的输出标签
-  $W^{[l]}{\in}\mathbb{R^{l \times (l-1)}}$代表第$[l]$层的权重矩阵
-  $b^{[l]}{\in}\mathbb{R^{l}}$代表第$[l]$层的偏差矩阵
-  $\hat{y}{\in}\mathbb{R^{n_y}}$是预测输出向量
  - 也可以用$a^{[L]}$表示

### **正向传播方程示例**

- $a=g^{[l]}(W_xx^{(i)}+b_1)=g^{[l]}(z_1)$ 其中，$g^{[l]}$代表第$l$层的激活函数
- $\hat{y}=softmax(W_hh+b_2)$

### **通用激活公式**

$a^{[l]}_j=g^{[l]}(z_j^{[l]})=g^{[l]}(\sum_kW_{jk}^{[l]}a_k^{[l-1]}+b_j^{[l]})$其中$j$为当前层的维度，$k$表示上一层的维度

### **损失函数**

$J(x,W,b,y)$或者$J(\hat{y},y)$ 

常见损失函数示例
- $J_{CE}(\hat{y},y)=-\sum_{i=0}^my^{(i)}log\hat{y}^{(i)}$ 
- $J_{1}(\hat{y},y)=-\sum_{i=0}^m|y^{(i)}-\hat{y}^{(i)}|$ 

## 深度学习图示

- 节点：代表输入、激活或者输出
- 边：代表权重或者误差

提供两种等效的示意图

### 详细的网络

![](http://www.ai-start.com/dl2017/images/fe20e8766346d4e8d212e792888dd6fb.jpg)

常用于神经网络的表示,为了更好的审美，我们省略了一些在边上的参数的细节(如$w_{ij}^{[l]}$和$b_i^{[l]}$等)。

### 简化网络

![](http://www.ai-start.com/dl2017/images/08a2f8fea114cbc35c70d45d03a34d52.jpg)

两层神经网络的更简单的表示。
## 相关资源

[deeplearning_ai_books](https://github.com/fengdu78/deeplearning_ai_books)

[https://mooc.study.163.com/university/deeplearning_ai#/c](https://mooc.study.163.com/university/deeplearning_ai#/c)

[**笔记在线阅读**](http://www.ai-start.com/dl2017)

## 专项课程概述

在**Cousera**的这一系列也叫做专项课程中,在第一门课中（**神经网络和深度学习**），你将学习神经网络的基础，你将学习神经网络和深度学习，这门课将持续四周，专项课程中的每门课将持续2至4周。在第一门课程中，你将学习如何建立神经网络（包含一个深度神经网络），以及如何在数据上面训练他们。在这门课程的结尾，你将用一个深度神经网络进行辨认猫。

接下来在第二门课中，我们将使用三周时间。你将进行深度学习方面的实践，学习严密地构建神经网络，如何真正让它表现良好，因此你将要学习超参数调整、正则化、诊断偏差和方差以及一些高级优化算法，比如**Momentum**和**Adam**算法，犹如黑魔法一样根据你建立网络的方式。第二门课只有三周学习时间。

在第三门课中，我们将使用两周时间来学习如何结构化你的机器学习工程。事实证明，构建机器学习系统的策略改变了深度学习的错误。举个例子：你分割数据的方式，分割成训练集、比较集或改变的验证集，以及测试集合，改变了深度学习的错误。所以最好的实践方式是什么呢？你的训练集和测试集来自不同的贡献度在深度学习中的影响很大，那么你应该怎么处理呢？如果你听说过端对端深度学习，你也会在第三门课中了解到更多，进而了解到你是否需要使用它，第三门课的资料是相对比较独特的，我将和你分享我们了解到的所有的热门领域的建立并且改良许多的深度学习问题。这些当今热门的资料，绝大部分大学在他们的深度学习课堂上面里面不会教的，我认为它会提供你帮助，让深度学习系统工作的更好。

在第四门课程中，我们将会提到卷积神经网络(**CNN(s)**)，它经常被用于图像领域，你将会在第四门课程中学到如何搭建这样的模型。

最后在第五门课中，你将会学习到序列模型，以及如何将它们应用于自然语言处理，以及其它问题。序列模型包括的模型有循环神经网络（**RNN**）、全称是长短期记忆网络（**LSTM**）。你将在课程五中了解其中的时期是什么含义，并且有能力应用到自然语言处理（**NLP**）问题。总之你将在课程五中学习这些模型，以及能够将它们应用于序列数据。比如说，自然语言就是一个单词序列。你也将能够理解这些模型如何应用到语音识别或者是编曲以及其它问题。

因此，通过这些课程，你将学习深度学习的这些工具，你将能够去使用它们去做一些神奇的事情，并借此来提升你的职业生涯。

……

你可能也听说过机器学习对于结构化数据和非结构化数据的应用，结构化数据意味着数据的基本数据库。例如在房价预测中，你可能有一个数据库，有专门的几列数据告诉你卧室的大小和数量，这就是结构化数据。或预测用户是否会点击广告，你可能会得到关于用户的信息，比如年龄以及关于广告的一些信息，然后对你的预测分类标注，这就是结构化数据，意思是每个特征，比如说房屋大小卧室数量，或者是一个用户的年龄，都有一个很好的定义。

相反非结构化数据是指比如音频，原始音频或者你想要识别的图像或文本中的内容。这里的特征可能是图像中的像素值或文本中的单个单词。

![](http://www.ai-start.com/dl2017/images/86a39d40cb13842cd6c06463cd9b4a83.png)

从历史经验上看，处理非结构化数据是很难的，与结构化数据比较，让计算机理解非结构化数据很难，而人类进化得非常善于理解音频信号和图像，文本是一个更近代的发明，但是人们真的很擅长解读非结构化数据。

……

作为一个具体的例子，神经网络方面的一个巨大突破是从**sigmoid**函数转换到一个**ReLU**函数，这个函数我们在之前的课程里提到过。

![](http://www.ai-start.com/dl2017/images/1a3d288dc243ca9c5a70a69799180c4a.png)

如果你无法理解刚才我说的某个细节，也不需要担心，可以知道的一个使用**sigmoid**函数和机器学习问题是，在这个区域，也就是这个**sigmoid**函数的梯度会接近零，所以学习的速度会变得非常缓慢，因为当你实现梯度下降以及梯度接近零的时候，参数会更新的很慢，所以学习的速率也会变的很慢，而通过改变这个被叫做激活函数的东西，神经网络换用这一个函数，叫做**ReLU**的函数（修正线性单元），**ReLU**它的梯度对于所有输入的负值都是零，因此梯度更加不会趋向逐渐减少到零。而这里的梯度，这条线的斜率在这左边是零，仅仅通过将**Sigmod**函数转换成**ReLU**函数，便能够使得一个叫做梯度下降（**gradient descent**）的算法运行的更快，这就是一个或许相对比较简单的算法创新的例子。但是根本上算法创新所带来的影响，实际上是对计算带来的优化，所以有很多像这样的例子，我们通过改变算法，使得代码运行的更快，这也使得我们能够训练规模更大的神经网络，或者是多端口的网络。即使我们从所有的数据中拥有了大规模的神经网络，快速计算显得更加重要的另一个原因是，训练你的神经网络的过程，很多时候是凭借直觉的，往往你对神经网络架构有了一个想法，于是你尝试写代码实现你的想法，然后让你运行一个试验环境来告诉你，你的神经网络效果有多好，通过参考这个结果再返回去修改你的神经网络里面的一些细节，然后你不断的重复上面的操作，当你的神经网络需要很长时间去训练，需要很长时间重复这一循环，在这里就有很大的区别，根据你的生产效率去构建更高效的神经网络。当你能够有一个想法，试一试，看效果如何。在10分钟内，或者也许要花上一整天，如果你训练你的神经网络用了一个月的时间，有时候发生这样的事情，也是值得的，因为你很快得到了一个结果。在10分钟内或者一天内，你应该尝试更多的想法，那极有可能使得你的神经网络在你的应用方面工作的更好、更快的计算，在提高速度方面真的有帮助，那样你就能更快地得到你的实验结果。这也同时帮助了神经网络的实验人员和有关项目的研究人员在深度学习的工作中迭代的更快，也能够更快的改进你的想法，所有这些都使得整个深度学习的研究社群变的如此繁荣，包括令人难以置信地发明新的算法和取得不间断的进步，这些都是开拓者在做的事情，这些力量使得深度学习不断壮大。





